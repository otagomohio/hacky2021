[
{
	"uri": "https://otagomohio.github.io/hacky2021/",
	"title": "Hacky Hour 2021",
	"tags": [],
	"description": "",
	"content": "Hacky Hour 2021 This is a resource for getting help on data analysis. This site will serve as the main base for the Hacky Hour 2021 help session, held monthly during the semester at the University of Otago Anatomy Department tearoom. Check here for announcements of new sessions, information from past sessions, and suggest new topics.\nNews First Hacky Hour will start on the 13th of July\n4:00 pm, Anatomy Department Tearoom (Lindo Ferguson Building, Room 237)\nThis year, the Hacky Hour meetings will alternate between the Anatomy and Zoology buildings. So, next session will be in two weeks in the Zoology building (room TBA).\nCheck back to this page for more information.\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/concepts/building_blocks/",
	"title": "Building blocks",
	"tags": [],
	"description": "",
	"content": "Most bioinformatic methods can be organised into a few basic concepts. I have organised these in terms of how the method influences the sequence files and how it modifies or what information is gathered from them. I use the term \u0026lsquo;sequence file\u0026rsquo; very broadly; it can encompass anything from raw reads to assembled contigs to scaffolds to whole organelle or chromosome assemblies. I have furthur divided these into two main classes: lower-level processes and higher-level processes. These are explained in detail below.\nLower-level building blocks I identify three major types of lower-level processes:\n Pattern searching \u0026ndash; finding patterns within a sequence Mapping \u0026ndash; pairwise alignment of a sequence against a database Clustering \u0026ndash; group together a collection of sequences based on identity  Each of these process types can encompass or be a part of many programs, and be applied to multiple sequence types\u0026ndash;as outlined above. Pattern searching is typical of early steps in most processing of raw reads, and includes adapter and primer removal, as well as quality filtering. However, it also is involved in many downstream processing steps, including searching for coding regions or CG islands on assembled contigs, or repeats across whole genome assemblies. Note that often these searches are looking for patterns based on existing references, such as adapter sequence files or repeat databases, but they can also involve looking for patterns not neccesarily dependent on any external reference, such as start and stop codons along a contig.\nFor Mapping processes, these involve searching whole sequences against some kind of reference database. As with Pattern searching, these can involve a BLAST search of short reads or assembled contigs or aligning RNA reads to a transcriptome. Kraken, HMM\nThese first two processes may seem like the same thing, and indeed they are more or less two sides of the same coin. But because they are usually very separate in pipelines I am keeping them distinct. I distinguish them by what is the subject and what is the object of the search: generally Pattern searching is looking for elements of a database somewhere along a sequence (database is subject, sequence is object), wherease Mapping is searching in a database for whole sequence reads (sequence is subject, database is object). Though for many searches, especially local alignments, matches to only part of the sequence may be found, it is usually the whole sequence that is the search query. Because of the nature of both of these (especially where pattern searching is looking for shorter and often, fragmentary patterns), generally the algorithms used for both are different.\nThe third type, Clustering, involves grouping or clustering a collection of sequences based on similarity. It is only comparing within a group of sequences, and does not generally involve any external references (of course, there are many hybrid clustering approaches that combine mapping and clustering, but often these happen in turn). Clustering methods are found across many bioinformatic pipelines and include kmer counting, OTU clustering (in metabarcoding), and dereplication. I include genome assembly here, though genome assemblies usually involve a wide range of steps, but building larger contigs from alignments of kmers is one of the core approaches. I also include multiple sequence alignment (MSA) here; while it is an alignment method, it is distinct from the Mapping alignment methods described above, as those generally involve pairwise comparisons\u0026ndash;aligning a single query separately to each of many subjects in a database. While a MSA is generated from the simultaneous alignment of multiple sequences\u0026ndash;at heart it is a very different approach.\nHere is a summary table that includes some examples of each method:\n   Pattern searching Mapping Clustering     finding patterns within a sequence pairwise alignment of a sequence against a database group together a collection of sequences based on identity   adapter/primer removal align reads to genome kmer counting   gene prediction BLAST search genome assembly   motif searching HMM search for protein domains dereplication   repeat finder create OTU table multiple sequence alignment         Next level: higher order processes From the basic processes, there are higher level methods that make statistical inferences about the results of the lower-level building blocks. For example, a phylogenetic analysis will try to infer relationships among samples in a multiple sequence alignment. Likewise, a principal component analysis (PCA) is often used to determine relationships among Operational Taxonomic Units (OTUs) resulting from a clustering analysis, such as USEARCH. I also include variant calling algorithms, such as GATK Haplotype Caller and FreeBayes, in this category, as they make inferences about statistically robust Single Nucleotide Polymorphisms (SNPs) and other variants resulting from mapping reads to a genome or transcriptome.\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/kura_reo/maori_informatics/",
	"title": "Maori Bioinformatic Phrases",
	"tags": [],
	"description": "",
	"content": "Following are bioinformatic-related phrases in Māori. Check the Māori Dictionary to look up more.\nrorohiko: computer\n{ roro: brain + hiko: flash, electronic }\ntūhonotanga: attachment, joining, link (can be used as a link to a webpage)\nThe \u0026ldquo;noun\u0026rdquo; version (indicated by the suffix -tanga) of the word tūhono: to join, bond, attach, connect\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/sessions/0713_data_management/",
	"title": "Data management",
	"tags": [],
	"description": "",
	"content": "Ludovic Dutoit, July 2021\nDescription This session covers the essential of data management for a project on NeSI. It starts with a bit of general thinking before moving into some NeSI specifics.\nGood habits  Always have your data in at least two places. Never edit your raw data. Use informative file names: filtered_data.txtis meaningless on its own. data_noMISSINGDATAremoved12lowqual.csv is much better. Avoid proprietary formats Have a readme file Your methods are easily transportable, and should bring anyone from raw data to results. Put your efforts there, Try to see files as checkpoints, not at the center of your structure. Use version control (i.e. github.com)  Tips and tricks for data management on NeSI If you do not have access to NeSI yet, apply here. Free, powerful, and maintained for you!\nThere are essentially 3 file systems on NeSI.\nThey are explained here.\nHow to check your usage?\nnn_storage_quota # check your quotas, specific to NeSI du -h # Show the size of every directory in the data structure. du -h | grep -E \u0026#34;^[0-9]+G\u0026#34; # check all the directories in the Gb range That is all good and well, but we sometimes end up with huge quantity of small files, can we have an infinity of those? No, files are assiocated to metadata, an empty file still takes some space for metadata on the system. That space is referred to as inodes, and they can also reach a quota.\nnn_storage_quota echo \u0026#34;Inode usage for $(pwd)\u0026#34; ; for d in `find -maxdepth 1 -type d | cut -d\\/ -f2 | grep -xv . | sort`; do c=$(find $d | wc -l) ; printf \u0026#34;$c\\t\\t- $d\\n\u0026#34; ; done ; printf \u0026#34;Total: \\t\\t$(find $(pwd) | wc -l)\\n\u0026#34; # This counts the inodes Where are those backups anyway?\ncd # Brings you home cd .snapshots ls -lh You got one week of backups there. Whether it is that exact folder or its content.\nSame inside the /nesi/project/ but not in \u0026lsquo;nobackup\u0026rsquo;\nLinks cd /nesi/nobackup/uooXXXXX # go to your nobackup directory mkdir tmp_data touch sample{0001..1000}.fastq Wow, we have a 1000 files here. You\u0026rsquo;d like to use them from your nobackup inside your normal project\ncd /nesi/project/uooXXXXX # go to your project directory Let\u0026rsquo;s create a link:\nln -s /nesi/nobackup/uooXXXXX/tmp_data/ . # create a soft link here. \u0026quot;.\u0026quot; is for here Have a look inside:\nld -lh ls -lh tmp_data/ Yay it is all here, we basically created a shortcut without duplicating the files.\nLet\u0026rsquo;s remove it now:\nrm -r tmp_data/ # -r is needed to remove a directory Is the original folder still there?\nls -lh /nesi/nobackup/uooXXXXX/tmp_data/ YES!\nStill not enough space? Ask support@nesi.org.nz first, if it is not enough, you can get data on the  high capacity storage (HCS) of the university too.\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/nesi/nesi_links/",
	"title": "NeSI Links",
	"tags": [],
	"description": "",
	"content": "The NeSI support page has good documentation for help in getting started and running many tools. This page has some of the most important links. The main page is here:\nNeSI support home page\nInitial set up This page provides guidelines for setting up your computer to access NeSI:\nStandard Terminal Setup\nJob scripts The biggest difference for doing analyses on NeSI versus your own computer is that you need to submit your analysis to a queue, so that the server can handle the demand. The queueing system that NeSI uses is the Slurm system. This entails writing a job script that details the resources you need for each analysis.\nHere is a guide to the basics:\nJob basics\nSLURM Best Practice\nHere is more documentation on the SLURM website:\nSlurm workload manager\nOne of the most important factors to getting your analyses run is to ask for the right resources (i.e. CPUs and RAM). This requires a balance: if you don\u0026rsquo;t ask for enough memory, for example, the job will die; if you ask for too much, the job will sit on the queue for a long time. Here is a guide to finding the right balance:\nFinding job efficiency\nThere are several partitions on NeSI for running analyses. Some are designed for very large jobs. Normally, the queueing system will pick the partition based on the resources you demand, but you can specify a partition. Here is a guide to the Mahuika partitions:\nMahuika Slurm Partitions\nTools on NeSI The job basics link has a guide to using the module command to find the program you need. Here is a list of all currently installed applications:\nSupported applications\nIf you use R for a lot of your data analysis, you probably use RStudio on your own computer. For running R on NeSI, you will have to make some changes. Here is a good article to help that adjustment:\nR on NeSI\nAnother way to run R on NeSI is through the JupyterHub. This is a great way to run Python, R, or other languages on NeSI, and is a good way to start using the server if you are not used to it. Here is a good guide:\nJupyter on NeSI\nAnd here is a direct link to the Hub:\nJupyterHub\nIn the future, RStudio server may be available on NeSI so that you can run large R jobs through NeSI with a familiar interface. Currently there is an experimental version of this, however, it is not in alpha mode and not recommended for beginners. You can read about it here.\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/basics/transferring_files/",
	"title": "Transferring Files",
	"tags": [],
	"description": "",
	"content": "Following are some example commands for transferring files and folders to and from the server. The NeSI support page also has good instructions.\nRemember:\n  You must have permissions to access the target folder on the server to which you wish to move your file\n  Your terminal must be on your computer to move files (that is, your pwd should be somewhere on your own computer)\n  Transfer files from your own computer to the server scp /path/to/file/FILENAME mahuika:/path/to/target/folder (Note the colon \u0026lsquo;:\u0026rsquo; after the server name.)\nIf your terminal is in the folder containing the file, you do not need the path:\nscp FILENAME mahuika:/path/to/target/folder Likewise, if you are transferring the file to your home folder on NeSI, then you do not need the path (although remember that your NeSI home folder does not have much space):\nscp FILENAME mahuika: Transferring to a subfolder in your home folder on NeSI:\nscp FILENAME mahuika:scripts/ If the folder is in another part of NeSI, then you need to put the full path:\nscp FILENAME mahuika:/nesi/project/uooXXXX/example_project/example_subfolder/ If you want to upload an entire folder (make sure you want to move all the files), just add the -r argument:\nscp -r /path/to/FOLDERNAME mahuika:/path/to/target/folder Note that this will create a folder called FOLDERNAME within the target folder.\nTransfer files from the server to your own computer In order to move files from NeSI or another server to your own computer, the order of paths is reversed:\nscp mahuika:/path/to/file/FILENAME /path/to/target/folder/ Note if your terminal is in the target folder (pwd), then you can just add a period at the end:\nscp mahuika:/path/to/file/FILENAME . The same rules apply for absolute paths. For example, if you are moving a file from your home folder on NeSI:\nscp mahuika:FILENAME /path/to/target/folder/ You can also substitute the ~ (tilde) for the root path on your computer (e.g. ~/ instead of /Users/hughcross/)\nAnd the same rules apply for downloading folders:\nscp -r mahuika:/nesi/project/uooXXXX/example_project/example_subfolder/ /path/to/target/folder rsync: A better way? You can also use the command line tool rsync to move files to and from the server. It has many uses, such as making regular backups to an external hard drive. rsync has some advantages over using scp, for example, you can update a folder you have already copied to the server, and it will only update the files that have changed (if it is a big folder, this can save lots of time). As well, if the transfer is interrupted, rsync will pick up where it left off. For purposes of archiving, when using rsync, the timestamp of your original files will be preserved, whereas with scp the date of the copied files will be the current one. This can be useful when backing up and preserving the original dates of your files.\nFor a single file\nrsync FILENAME mahuika:/path/to/target/folder It is better to use the -a option, as that will preserve time stamp and permissions, etc. Here I have also added the -v option (\u0026ndash;verbose) which will output the status:\nrsync -av FILENAME mahuika:/path/to/target/folder For syncing folders it is the same command:\nrsync -av /path/to/folder mahuika:/path/to/target/folder And, the same thing transferring from the server to your computer:\nrsync -av mahuika:/path/to/folder ~/Documents If you add the --delete option, any files on the target that are not on the source folder will be deleted. To avoid accidentally deleting any precious files, it is advised to use the --dry-run command first, which will show you what would happen without actually doing anything:\nrsync -av /path/to/source/folder --dry-run --delete scripts mahuika:/path/to/target/folder Once you have checked it will do what you want, you can run without the dry-run option\nrsync -av /path/to/source/folder --delete scripts mahuika:/path/to/target/folder As mentioned, you can do this to back up to an external hard drive:\nrsync -av /path/to/source/folder /Volumes/NAME_OF_EXTERNAL_HD/target/folder "
},
{
	"uri": "https://otagomohio.github.io/hacky2021/bugs/troubleshooting/",
	"title": "Troubleshooting errors",
	"tags": [],
	"description": "",
	"content": "What went wrong? How to spot your mistakes and fix them We have all been there: You have followed all the right steps, have triple-checked the commands, and somehow it doesn’t work; or, you think it works but then the results don\u0026rsquo;t make sense; or, nonsensical error messages seem to scream at you. Next Wednesday we will share some of our own mistakes, errors, FUs, boners, mishaps, catastrophes, and how we fixed them (at least sometimes). And we will try to explain how sometimes even the best designed pipelines don’t survive the first dataset.\nWhy pencils have erasers Here are a few links that list common mistakes in bioinformatics:\nCommon stupid mistakes in bioinformatics\nMore common mistakes in bioinformatics\nHow not to be a bioinformatician\nLinks for getting help The first step that most of us do is google the error message. Sometimes the trick is how to phrase the search. The following websites often come up when you google an error message, but here are the main pages.\nStackoverflow\nSEQanswers\nBiostars\nGoogle Groups and GitHub Two of the best places to keep up on what is going on with a particular program or software package are Google Groups and GitHub. Be sure to check if there is a google group for the software that is giving you problems. Feel free to post your questions; often the developer themself will post an answer and respond to questions. Just do a search on the site to make sure someone else hasn\u0026rsquo;t already had this issue. If the software is on GitHub, then you can check the Issues tab and browse or search there for a similar problem (sometimes googling misses these, depending on how you phrase the search).\nJump back to main Hacky Hour page\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/basics/",
	"title": "Basics",
	"tags": [],
	"description": "",
	"content": "basic terminal knowhow \u0026ldquo;The journey of a thousand miles begins with a single step\u0026rdquo;\n-Lao Tzu\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/sessions/",
	"title": "Sessions",
	"tags": [],
	"description": "",
	"content": "Help Sessions \u0026ldquo;All paths lead to the same goal: to convey to others what we are\u0026rdquo;\n-Pablo Neruda\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/nesi/",
	"title": "NeSI",
	"tags": [],
	"description": "",
	"content": "Using the NeSI server \u0026ldquo;That is the exploration that awaits you!\nNot mapping stars and studying nebula,\nbut charting the unknown possibilities of existence.\u0026quot;\n-Leonard Nimoy\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/sessions/0810_vscode_tricks/",
	"title": "Text Editor Hacks",
	"tags": [],
	"description": "",
	"content": "Presented by Hugh Cross\nGetting more out of your text editor This session will highlight a couple of ways to utilise your text editor in ways you may not have thought possible. We will use the popular text editor Visual Studio Code (VS Code) for this session, but the tools we show you today are available in some fashion on other text editors, including Sublime Text and Atom.\nInstalling VS Code Extensions When you open Visual Studio Code there are several tabs on the side that will expand to show different options. There are hundreds of tools available for VS Code, and these can be accessed by clicking on the Extensions tab:\nWhen the tab opens, type the extension you are looking for in the Search box at the top. Extensions you already have installed will be listed there. The first extension we are going to load is called Remote - SSH. Type that in the search box and then select this one from the search results. A description and details of the extension will appear. Click on Install at the top of the extension description to install it (in the image, it says Uninstall as this had already been loaded on my VS Code).\nThe next extension we will need for today is SendToTerminal+. Search and install this extension the same way:\nYou can look for other extensions as well, including color themes that do not come with the standard install. You can also explore extensions you may want on the webpage.\nEditing files on the server with VS Code The Remote - SSH extension allows you to open and edit files on NeSI or another remote server that you have access to. If you are not used to terminal text editors like Nano or Vim, this will help you get going on NeSI (though those text editiors are still useful tools to know). In order to use this tool, your computer will already need to be set up to log in to NeSI; this includes having a config file setup in your .ssh folder as per the instructions on the NeSI webpage. Once your computer is set up, you can log an open window of VS Code to NeSI. First, click on the new tab on the side, which is the Remote Explorer:\nThen, right click on mahuika (or whatever server you are accessing) in the panel that opens. Select either Connect to Host in Current Window or Connect to Host in New Window. If you are not already logged in to NeSI on a terminal, you will be prompted to enter password and security code [HINT: It is strongly recommended to log in with a terminal before logging in with VS Code; I found it timed me out often from VS Code, but if you are already logged in with a terminal, VS Code will skip the authentication.].\nWorking remotely with VS Code, You will see the status at the bottom of the VS Code window tell you when you are logged in to a remote server. Once you are in, you can then open a folder anywhere you have access to on the server. Click on the folder Explorer tab on the top and then click on \u0026lsquo;Open Folder\u0026rsquo;. A text entry box will appear, and you can enter the path to the folder you want to open:\nOnce on, you will see all the files and subfolders on the left-hand side of the window. You can click on most text files to open and edit them:\nThis will enable you to edit job scripts, or other files. You can even open some PDF and other image files, but this capacity is limited for now.\nUsing SendToTerminal+ to document your work Now we will employ the other extension we installed. First, we need a terminal. VS Code comes with built-in terminals. Click on Terminal \u0026gt; New Terminal, from the main menu, and a new terminal will open within the window. If the window is logged on to NeSI, then the terminal will automatically be on NeSI. (You can of course, open windows offline in other windows as well.)\nThe terminal should open up in the same folder as you opened in Explorer above. Now, you can create a new text file by right-clicking on the folder where you want the file to be and select New File, or with the keyboard shortcut Cmd + n. In the text file, try typing a bash command, like pwd, on a line. Then, with the cursor on that line, hold down the option and shift keys and then hit enter. The command should appear on the terminal below, with the output just below, as if you had typed it in the terminal itself:\nI use this extension to document my work as I go. Instead of entering the commands directly on the terminal, and then forgetting exactly what I did later, this way you will document what you are doing and can refer to it later. You can add comments to the commands (\u0026lsquo;this didn\u0026rsquo;t this work\u0026rsquo;, \u0026lsquo;oh crap, why won\u0026rsquo;t this work\u0026rsquo;, etc). You can also select multiple lines on the text file, like for running a for loop, and enter all the lines as one command.\nThe SendToTerminal+ extension will work on Python and R as well. You can load the R module, enter R, and then send R commands to the console, in much the same way you would on RStudio. I found graphics did not work quite as well; there is some limited support to visualise the graphics, but it is better to do this with RStudio, or JupyterHub if you are on NeSI. However, if you want to run a few lines of R to fix a table or run some stats, it works.\nFor Python, load a Python module, and then in the terminal enter python or ipython for enhanced capability. Now you can send python code to the terminal to run. Again, JupyterHub would be much better for running Python code on NeSI, but sometimes I need to just run a few lines of Python to fix a file or something, and this way, the entire session\u0026ndash;with bash, R, and Python\u0026ndash;can be in a single document.\nOf course, SendToTerminal+ works great for regular VS Code on your own computer, or you can even log in to NeSI just with the terminal (just enter ssh mahuika) and keep the VS Code and the text file documentation on your computer.\nAcknowledgements Thanks to Dinindu Senanayake at NeSI for showing me the Remote - SSH extension\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/bugs/",
	"title": "Debugging",
	"tags": [],
	"description": "",
	"content": "How to find and solve common problems \u0026ldquo;Give a man a fish and feed him for a day.\nDon\u0026rsquo;t teach a man to fish and feed yourself.\nHe\u0026rsquo;s a grown man. And fishing\u0026rsquo;s not that hard\u0026rdquo;\n-Ron Swanson\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/basics/tmux_basics/",
	"title": "Tmux basics",
	"tags": [],
	"description": "",
	"content": "tmux, or \u0026lsquo;Terminal Multiplexer\u0026rsquo;, is a very useful tool for using the command line. It has many, many tools, but the most important feature for starting out is that you can use it to run programs in the background while you are logged off. That is, you can start a long job, then log out of the server and go home, and tmux will keep the job running. Most of you are familiar with the program screen to accomplish this, but tmux is much more powerful. Below are a few commands to get you started. Once you are used to using it you can begin to explore more options. In the future we will try to keep this page updated with additional tips and tricks, but for now, here is one (of many) cheat sheets available online:\ntmuxcheatsheet.com\nStarting and detaching from a session New Session To start an unnamed session, you can just enter the name of the program at the prompt:\ntmux I suggest you give a session a name when you start it, which will make it easier to keep track once you have multiple sessions going:\ntmux new -s name_of_session Detach from a Session To \u0026lsquo;detach\u0026rsquo; or leave a session, you hold down the Control key and the \u0026lsquo;b\u0026rsquo; keys at the same time, release, then press the \u0026rsquo;d' key:\n\u0026lsquo;\u0026lsquo;\u0026lsquo;bash Ctrl-b d '\u0026rsquo;\u0026rsquo;\nNote that the Control and b key action is referred to as a \u0026lsquo;prefix\u0026rsquo; or \u0026lsquo;chord\u0026rsquo;, depending on the tmux guide. This action can be represented differently in different places. For example, on the link above, it is listed as \u0026ldquo;Ctrl + b d. The prefix starts a lot of actions in tmux, but once you are used to it you will find it is easy.\nGo back to existing session If you have started a big job in tmux, and wish to re-enter it, or \u0026lsquo;attach\u0026rsquo;, it is:\ntmux attach or\ntmux a This will go back to the last session. If you have named your session, you can specify this:\ntmux a -t name_of_session Listing active sessions You can list all your tmux sessions, even if you haven\u0026rsquo;t named them:\ntmux ls Note naming your sessions will make it easier to know which you have. If you haven\u0026rsquo;t named them, then they will be numbered. The names (or numbers) of sessions will be the first item when you list them\nDeleting active sessions If you are finished with the job, and no longer need the session, then you can delete, or \u0026lsquo;kill\u0026rsquo; it:\ntmux kill-session -t name_of_session If you do not specify a name of a session (no -t argument), then tmux will kill the last session made.\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/nesi/collaboration_server/",
	"title": "Collaboration on NeSI",
	"tags": [],
	"description": "",
	"content": "The Linux operating system was designed for multiple users, which makes it ideal for a server environment, and allows you to share your scripts and files with your colleagues. Here are some links and quick tips to getting your work going and letting others access your files.\nQuick tip: getting your script running Once you have written your script, in order to run it you have to make it executable. This is accomplished with one simple command:\nchmod a+x To break down the above command: chmod means \u0026lsquo;change mode\u0026rsquo;, and is the main command to change permissions. The a refers to all: that you are giving permission for anyone to use this script; the + is to add a permission, and the x is to make the file executable.\nFor a full explanation of all the permission symbols, see this link\nMaking your script available system wide Once you have your script, you can make it available so the computer can find it anywhere on the system. If you are submitting a job (slurm) script on NeSI, you can add something like the following to the slurm script:\nexport PATH='/nesi/project/uoo5555555/scripts/':$PATH The script should be able to then find the script and run it (don\u0026rsquo;t forget to make it executable).\nSymbolic links If you have a folder that is already in the Path (e.g. /usr/local/bin), you can add a symbolic link that adds the script to the existing path:\nln -s /path/to/file /path/to/symlink Sharing files and folders on the server If you are using NeSI, the server is set up so that anyone on the same project can share files and folders in that project directory. If you need to join a project, go to this link for instructions.\nOn other servers, the defaults may be different, and you will have to change the permissions to add a user to one of your directories. A single command can allow everyone in your group access:\nsudo chmod -R 775 /path/to/directory Now run ls -l to see the change in permissions\nThe only issue is that future files that you create will assign the group to the default for your user account. There are two steps to keep all files written in this directory assigned to the group:\numask 0002 sudo chmod g+s /path/to/directory The umask command is explained in detail here and here, and the special permissions command (chmod g+s) is explained here.\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/bugs/getting_help/",
	"title": "Getting help",
	"tags": [],
	"description": "",
	"content": "We would like to increase communication among the group. Of course, you are always free to bring your issues and questions to Hacky Hour, but problems occur at random, often inconvenient times. Therefore we are exploring ways for you to post issues to us, and provide a flexible way for everyone to comment. Some suggested options I have heard include: Slack, Wikis, GitHub, and even Twitter. We can discuss all of these, but for this session we start with GitHub.\nPosting issues on a GitHub repository To post an issue on any GitHub repo, you will need a GitHub account. Once you have that, go to this link:\nHacky Hour Issues\nIt is always good practice to check the existing issues (even closed issues) to see if your problem has been addressed.\nIf you find a post that is relevant, you just click on it, and you can add your own comment to the bottom. You can paste in snippets of code, or add files and links. The comment box is Markdown-compatible, so you can format your text with markdown.\nIf you want to be notified about any changes to this issue, click on Notifications box to the right\nIf there is no relevant issue, then you can start a new issue, just click on the green box labeled New issue. You will then receive an email when anyone responds on this post.\nYou can also post and search issues on Google Groups pages "
},
{
	"uri": "https://otagomohio.github.io/hacky2021/sessions/0907_snakemake/",
	"title": "Snakemake Tutorial",
	"tags": [],
	"description": "",
	"content": "Written and presented by Julie Blommaert\nWhat the heck is a snakemake? Is it a snake that likes to make cake? It\u0026rsquo;s a workflow manager: What is a workflow manager? Examples of workflow graphs produced by Snakemake:\nBut what\u0026rsquo;s in it for me?  Reproducibility  Not just good for science, but good for future you   Record keeping Easy to share Adaptability  How does it work then?  Each step is called a \u0026ldquo;rule\u0026rdquo; Each rule is defined by input and output files, and a task Snakemake can then figure out how the tasks are connected and if any files are missing or have been updated Snakemake builds on Python syntax, but you can use other languages within it (e.g. bash, R)  An example with explanations:\nCode example of two \u0026lsquo;rules\u0026rsquo;:\nrule genome_admin: \u0026quot;\u0026quot;\u0026quot; Shorten the fasta headers,make a blastdb, and fasta index the genome \u0026quot;\u0026quot;\u0026quot; params: genome = config[\u0026quot;genome_name\u0026quot;] input: assembly=expand(\u0026quot;assemblies/{name}.fasta\u0026quot;,name=config[\u0026quot;genome_name\u0026quot;]) output: index =expand(\u0026quot;assemblies/{name}.fasta.fai\u0026quot;,name=config[\u0026quot;genome_name\u0026quot;]) shell: \u0026quot;\u0026quot;\u0026quot; samtools faidx {input.assembly} makeblastdb -in {input.assembly} -parse_seqids -dbtype nucl \u0026quot;\u0026quot;\u0026quot; rule get_genome_fasta: \u0026quot;\u0026quot;\u0026quot; Retrieve the sequence in fasta format for a genome. \u0026quot;\u0026quot;\u0026quot; threads: 1 params: genomelink = config[\u0026quot;ncbi_link\u0026quot;] output: outfile=expand(\u0026quot;assemblies/{name}.fasta\u0026quot;,name=config[\u0026quot;genome_name\u0026quot;]) shell: \u0026quot;\u0026quot;\u0026quot; wget {params.genomelink} -O temp.gz gunzip temp.gz cat temp| awk '{{if($1~\u0026quot;\u0026gt;\u0026quot;){{printf($1\u0026quot;\\\\n\u0026quot;)}}else{{print $0}}}}'\u0026gt; {output.outfile} \u0026quot;\u0026quot;\u0026quot; The specific files on which the Snakemake workflow operates are determined by a config file:\nconfig.yml:\ngenome_name: cygAtr1 ncbi_link: https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/013/377/495/GCF_013377495.1_Cygnus_atratus_primary_v1.0/GCF_013377495.1_Cygnus_atratus_primary_v1.0_genomic.fna.gz masking_lib: /proj/sllstore2017073/private/RepeatLibs/lycPyr2_rm2.1_merged.lib The goal? a larger workflow:\nLinks More comprehensive tutorials:\n NBIS Workshop  Data Carpentry Lesson on Snakemake Snakemake Tutorial Snakemake Documentation Page  Many options to organise your workflow  Bash scripts (you have to start some place) Galaxy (web-based workflow) Nextflow Common Workflow Language (CWL)   Image from NBIS Workshop  "
},
{
	"uri": "https://otagomohio.github.io/hacky2021/concepts/",
	"title": "Concepts",
	"tags": [],
	"description": "",
	"content": "Basic concepts of bioinformatics \u0026ldquo;Let\u0026rsquo;s be straight here. If we find something we can\u0026rsquo;t understand we like to call it something you can\u0026rsquo;t understand, or indeed pronounce\u0026rdquo;\n-Douglas Adams\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/kura_reo/",
	"title": "Kura Reo",
	"tags": [],
	"description": "",
	"content": "Kura Reo \u0026ldquo;Poipoia te kakano kia puawai\u0026rdquo;\n(\u0026quot;Nurture the seed and it will blossom\u0026quot;)\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/basics/regular_expressions/",
	"title": "Regular expressions",
	"tags": [],
	"description": "",
	"content": "For bioinformatics, it is critical to know how to use at least the basics of regular expressions (regex). There are regex that you use all the time, but most of us will look them up more often than not. Following are some links to online tutorials and cheatsheets to guide you the next time you need to use them (it will probably be sooner than you think!\nTutorials and Cheatsheets RexEgg Tutorial\nRexEgg Cheat Sheets\nRegexOne: an interactive tutorial\nRegular-Expressions.info. The name says it all! Also has links for grep and other languages (see below).\nRyans Tutorials\nA very basic regex cheatsheet\nA more thorough cheatsheet\ngrep: find anything the command line program grep, which stands for global regular expression print, can search files and folders for patterns, which includes regular expressions.\nRegex.info site\nUsing regular expressions in grep\ngrep tutorial\nRyans Tutorial on grep and regex\nIntro to grep\nLinks for other languages RexEgg for Python\nregex.info for Python\nregex for Python cheatsheet\nR documentation for regex\nA fairly comprehensive regex primer for the R language\nRStudio.com regex cheatsheet (pdf)\nFor R tidyverse users, here is a link to R for Data Science chapter section on regex, and here is the stringr package regex page.\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/basics/shell_scripting_basics/",
	"title": "Shell scripting basics",
	"tags": [],
	"description": "",
	"content": "Once you move beyond basic shell commands, and the process becomes increasingly complicated, you are going to want the commands into a shell script. There are many good tutorials online to get you started. The Software Carpentry website includes some good tips on their Unix Shell lesson (chapters 5 and 6). As well, Data Carpentry has a good introduction on their lesson: Introduction to the Command Line for Genomics. For the lesson here, we will take you through some basics to get you started.\nSimple script In a text editor, add the following:\n#!/usr/bin/env bash # a first script echo 'Hello World' The first line is called the Shebang, which tells the script where the BASH interpreter is. It is possible to run without this line, but your script may not work in all environments (see Here and Here for more details than you would ever want to know). The next line that starts with the # is a comment, and is ignored by BASH, but can help tell the reader what is going on in the script. The last line just has the command to output \u0026ldquo;Hello World\u0026rdquo; to the screen.\nIn order to run any new shell script, you have to make it executable:\nchmod a+x and now it should run. You just have to do this the first time you run it.\nA little less simple: running other programs The following script fastq2fasta.sh will convert one of our example fastq files to a fasta format\n#!/usr/bin/env bash # running another program with the shell: seqtk # This command will convert the fastq file to a fasta file seqtk seq -A example_data/e4485061.924a.4f09.9d7e.bfefc780d1a8_11_L001_R1_001.fastq \u0026gt; example_data/sample1.fasta Repeating the same command: Looping The above script works fine if we want to just do one file (but then you would hardly need a script for one command for one file), but if we want to do the same command on multiple files, we can set up a for loop:\nShell script: fastq2fasta_loop.sh\n#!/usr/bin/env bash # running another program with the shell: seqtk # This command will convert multiple fastq files to the fasta format cd example_data for fq in *.fastq do seqtk seq -A $fq \u0026gt; $fq\\.fasta done The \\ in the command is called an escape. That is so the program doesn\u0026rsquo;t confuse what variable you are calling.\nPlaying with variables Okay, now we can loop through multiple files and run the same command. The only thing with the above script is that now all of the fasta files end in \u0026ldquo;.fastq.fasta\u0026rdquo;, which doesn\u0026rsquo;t look so nice. We can adjust the variable that the for loop is iterating through to adjust the output name:\nfastq2fasta_loop_rename.sh\n#!/usr/bin/env bash # running another program with the shell: seqtk # This command will convert multiple fastq files to the fasta format cd example_data for fq in *.fastq do sampleName=$(echo $fq | cut -f 1 -d '.') echo $sampleName seqtk seq -A $fq \u0026gt; $sampleName\\.fasta done Looping from a file If we don\u0026rsquo;t want to run all the files in the folder, we can make a list and run the loop from there. One way to do this is a While loop:\nfastq2fasta_while_loop_rename.sh\n#!/usr/bin/env bash # running another program with the shell: seqtk # This command will convert multiple fastq files to the fasta format cd example_data cat fqlist | while read line do sampleName=$(echo $line | cut -f 1 -d '.') echo $sampleName seqtk seq -A $line \u0026gt; $sampleName\\.fasta done We can also take more complicated things from a file, for example a table with old and new names, and use these with our variable slicing to give more sensible names to our files:\nfastq2fasta_loop_newName.sh\n#!/usr/bin/env bash # running another program with the shell: seqtk # This command will convert multiple fastq files to the fasta format cd example_data cat fqTable | while read line do currentName=$(echo \u0026quot;$line\u0026quot; | cut -f 1) newName=$(echo \u0026quot;$line\u0026quot; | cut -f 2) echo $currentName echo $newName seqtk seq -A $currentName \u0026gt; $newName\\.fasta # can also use to just change names: #mv $currentName $newName done Are you jazzed about for loops and want even more speed? We\u0026rsquo;ve got a small guide on how to parallelize these loops\n"
},
{
	"uri": "https://otagomohio.github.io/hacky2021/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://otagomohio.github.io/hacky2021/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]